{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9cac58d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef create_sample_data():\\n    \"\"\"Create sample data in long format for demonstration\"\"\"\\n    np.random.seed(42)\\n    dates = pd.date_range(\\'2022-01-01\\', periods=36, freq=\\'MS\\')\\n\\n    # Create different types of SKUs\\n    skus_data = []\\n\\n    # Seasonal SKU\\n    seasonal_pattern = 50 + 20 * np.sin(2 * np.pi * np.arange(36) / 12)\\n    seasonal_orders = seasonal_pattern + np.random.normal(0, 5, 36)\\n\\n    # Growth SKU\\n    growth_trend = 100 + 5 * np.arange(36)\\n    growth_orders = growth_trend + np.random.normal(0, 10, 36)\\n\\n    # Volatile SKU\\n    volatile_base = 200 + np.random.normal(0, 50, 36)\\n    volatile_orders = np.abs(volatile_base)\\n\\n    # Stable SKU\\n    stable_orders = 150 + np.random.normal(0, 8, 36)\\n\\n    # Decline SKU\\n    decline_trend = 300 - 5 * np.arange(36)\\n    decline_orders = decline_trend + np.random.normal(0, 15, 36)\\n\\n    # Create long format data\\n    all_data = []\\n\\n    sku_patterns = {\\n        \\'SKU001\\': seasonal_orders,\\n        \\'SKU002\\': growth_orders,\\n        \\'SKU003\\': volatile_orders,\\n        \\'SKU004\\': stable_orders,\\n        \\'SKU005\\': decline_orders\\n    }\\n\\n    for sku, orders in sku_patterns.items():\\n        for i, date in enumerate(dates):\\n            all_data.append({\\n                \\'SKU\\': sku,\\n                \\'date\\': date.strftime(\\'%Y-%m-%d\\'),\\n                \\'actual_orders\\': max(0, orders[i])  # Ensure non-negative\\n            })\\n\\n    return pl.DataFrame(all_data)\\n\\n# Demo\\nif __name__ == \"__main__\":\\n    # Create sample data in long format\\n    sample_data = create_sample_data()\\n    print(\"Sample data structure:\")\\n    print(sample_data.head())\\n\\n    # Initialize segmentation\\n    segmenter = TimeSeriesSegmentation(sample_data)\\n\\n    # Calculate features\\n    features = segmenter.calculate_sku_features()\\n    print(\"\\nSKU Features:\")\\n    for sku, feat in features.items():\\n        print(f\"{sku}: CV={feat[\\'cv\\']:.2f}, Seasonality={feat[\\'seasonality_strength\\']:.2f}, Growth={feat[\\'growth_rate\\']:.2f}\")\\n\\n    # Segment SKUs\\n    segments = segmenter.segment_skus(method=\\'combined\\')\\n    print(\"\\nSegments:\")\\n    for sku, segment in segments.items():\\n        print(f\"{sku}: {segment}\")\\n\\n    # Analyze segments\\n    analysis = segmenter.analyze_segments()\\n    print(\"\\nSegment Analysis:\")\\n    for segment, chars in analysis.items():\\n        print(f\"{segment}: {chars[\\'sku_count\\']} SKUs, Avg Volume: {chars[\\'avg_volume\\']:.0f}\")\\n\\n    # Get model recommendations\\n    recommendations = segmenter.recommend_forecasting_models()\\n    print(\"\\nModel Recommendations:\")\\n    for segment, models in recommendations.items():\\n        print(f\"{segment}: {models}\")\\n\\n    # Plot segments\\n    segmenter.plot_segments()'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import polars as pl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from scipy.signal import find_peaks\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class TimeSeriesSegmentation:\n",
    "    \"\"\"\n",
    "    Comprehensive time series segmentation for order forecasting\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data, sku_col='SKU', date_col='date', value_col='actual_orders'):\n",
    "        \"\"\"\n",
    "        Initialize with data in long format:\n",
    "        - sku_col: column name for SKU identifiers\n",
    "        - date_col: column name for dates (should be monthly)\n",
    "        - value_col: column name for order quantities\n",
    "        \"\"\"\n",
    "        self.raw_data = data\n",
    "        self.sku_col = sku_col\n",
    "        self.date_col = date_col\n",
    "        self.value_col = value_col\n",
    "        \n",
    "        # Convert long format to wide format for analysis\n",
    "        self.data = self._prepare_data()\n",
    "        self.sku_features = {}\n",
    "        self.segments = {}\n",
    "        self.segment_characteristics = {}\n",
    "    \n",
    "    def _prepare_data(self):\n",
    "        \"\"\"Convert long format polars dataframe to wide format pandas dataframe\"\"\"\n",
    "        # Convert date column to datetime if not already\n",
    "        df = self.raw_data.with_columns([\n",
    "            pl.col(self.date_col).str.to_datetime().alias(self.date_col)\n",
    "        ])\n",
    "        \n",
    "        # Sort by SKU and date\n",
    "        df = df.sort([self.sku_col, self.date_col])\n",
    "        \n",
    "        # Pivot to wide format\n",
    "        wide_df = df.pivot(\n",
    "            index=self.date_col,\n",
    "            columns=self.sku_col,\n",
    "            values=self.value_col,\n",
    "            aggregate_function='sum'\n",
    "        )\n",
    "        \n",
    "        # Convert to pandas for easier time series analysis\n",
    "        wide_pandas = wide_df.to_pandas()\n",
    "        wide_pandas = wide_pandas.set_index(self.date_col)\n",
    "        \n",
    "        # Ensure monthly frequency\n",
    "        wide_pandas = wide_pandas.asfreq('MS')\n",
    "        \n",
    "        return wide_pandas\n",
    "        \n",
    "    def calculate_sku_features(self):\n",
    "        \"\"\"Calculate comprehensive features for each SKU\"\"\"\n",
    "        features = {}\n",
    "        \n",
    "        for sku in self.data.columns:\n",
    "            series = self.data[sku].dropna()\n",
    "            \n",
    "            if len(series) < 12:  # Need at least 12 months\n",
    "                continue\n",
    "                \n",
    "            # Basic statistics\n",
    "            mean_volume = series.mean()\n",
    "            std_volume = series.std()\n",
    "            cv = std_volume / mean_volume if mean_volume > 0 else np.inf\n",
    "            \n",
    "            # Trend analysis\n",
    "            x = np.arange(len(series))\n",
    "            slope, intercept, r_value, p_value, std_err = stats.linregress(x, series)\n",
    "            trend_strength = abs(r_value)\n",
    "            trend_direction = 1 if slope > 0 else -1 if slope < 0 else 0\n",
    "            \n",
    "            # Seasonality detection\n",
    "            seasonality_strength = self._detect_seasonality(series)\n",
    "            \n",
    "            # Growth/decline patterns\n",
    "            growth_rate = self._calculate_growth_rate(series)\n",
    "            volatility = self._calculate_volatility(series)\n",
    "            \n",
    "            # Volume characteristics\n",
    "            volume_category = self._categorize_volume(mean_volume)\n",
    "            \n",
    "            # Lifecycle stage\n",
    "            lifecycle_stage = self._detect_lifecycle_stage(series)\n",
    "            \n",
    "            # Structural breaks\n",
    "            break_points = self._detect_structural_breaks(series)\n",
    "            \n",
    "            features[sku] = {\n",
    "                'mean_volume': mean_volume,\n",
    "                'cv': cv,\n",
    "                'trend_strength': trend_strength,\n",
    "                'trend_direction': trend_direction,\n",
    "                'seasonality_strength': seasonality_strength,\n",
    "                'growth_rate': growth_rate,\n",
    "                'volatility': volatility,\n",
    "                'volume_category': volume_category,\n",
    "                'lifecycle_stage': lifecycle_stage,\n",
    "                'break_points': len(break_points),\n",
    "                'series_length': len(series)\n",
    "            }\n",
    "        \n",
    "        self.sku_features = features\n",
    "        return features\n",
    "    \n",
    "    def _detect_seasonality(self, series):\n",
    "        \"\"\"Detect seasonal patterns using autocorrelation\"\"\"\n",
    "        if len(series) < 24:\n",
    "            return 0\n",
    "        \n",
    "        # Calculate autocorrelation at lag 12 (annual seasonality)\n",
    "        autocorr_12 = series.autocorr(lag=12)\n",
    "        \n",
    "        # Also check for other seasonal patterns\n",
    "        seasonal_lags = [12]  # Quarterly, semi-annual, annual\n",
    "        max_autocorr = 0\n",
    "        \n",
    "        for lag in seasonal_lags:\n",
    "            if len(series) > lag * 2:\n",
    "                autocorr = abs(series.autocorr(lag=lag))\n",
    "                if not np.isnan(autocorr):\n",
    "                    max_autocorr = max(max_autocorr, autocorr)\n",
    "        \n",
    "        return max_autocorr\n",
    "    \n",
    "    def _calculate_growth_rate(self, series):\n",
    "        \"\"\"Calculate compound annual growth rate\"\"\"\n",
    "        if len(series) < 12:\n",
    "            return 0\n",
    "        \n",
    "        start_val = series.iloc[:6].mean()  # First 6 months average\n",
    "        end_val = series.iloc[-6:].mean()   # Last 6 months average\n",
    "        \n",
    "        if start_val <= 0:\n",
    "            return 0\n",
    "        \n",
    "        periods = len(series) / 12  # Years\n",
    "        growth_rate = (end_val / start_val) ** (1/periods) - 1\n",
    "        return growth_rate\n",
    "    \n",
    "    def _calculate_volatility(self, series):\n",
    "        \"\"\"Calculate volatility as rolling CV\"\"\"\n",
    "        if len(series) < 6:\n",
    "            return 0\n",
    "        \n",
    "        rolling_mean = series.rolling(window=6, min_periods=3).mean()\n",
    "        rolling_std = series.rolling(window=6, min_periods=3).std()\n",
    "        rolling_cv = rolling_std / rolling_mean\n",
    "        \n",
    "        return rolling_cv.mean()\n",
    "    \n",
    "    def _categorize_volume(self, mean_volume):\n",
    "        \"\"\"Categorize volume into high/medium/low\"\"\"\n",
    "        # This should be adapted based on your specific volume distribution\n",
    "        if mean_volume < 100:\n",
    "            return 'low'\n",
    "        elif mean_volume < 1000:\n",
    "            return 'medium'\n",
    "        else:\n",
    "            return 'high'\n",
    "    \n",
    "    def _detect_lifecycle_stage(self, series):\n",
    "        \"\"\"Detect product lifecycle stage\"\"\"\n",
    "        if len(series) < 12:\n",
    "            return 'unknown'\n",
    "        \n",
    "        # Split into thirds\n",
    "        third = len(series) // 3\n",
    "        first_third = series.iloc[:third].mean()\n",
    "        second_third = series.iloc[third:2*third].mean()\n",
    "        last_third = series.iloc[2*third:].mean()\n",
    "        \n",
    "        # Growth patterns\n",
    "        if first_third < second_third < last_third:\n",
    "            return 'growth'\n",
    "        elif first_third > second_third > last_third:\n",
    "            return 'decline'\n",
    "        elif first_third < second_third and second_third > last_third:\n",
    "            return 'maturity'\n",
    "        else:\n",
    "            return 'stable'\n",
    "    \n",
    "    def _detect_structural_breaks(self, series):\n",
    "        \"\"\"Simple structural break detection using rolling statistics\"\"\"\n",
    "        if len(series) < 12:\n",
    "            return []\n",
    "        \n",
    "        # Calculate rolling mean and std\n",
    "        window = 6\n",
    "        rolling_mean = series.rolling(window=window).mean()\n",
    "        rolling_std = series.rolling(window=window).std()\n",
    "        \n",
    "        # Find significant changes\n",
    "        mean_changes = rolling_mean.diff().abs()\n",
    "        std_changes = rolling_std.diff().abs()\n",
    "        \n",
    "        # Identify breaks (simplified approach)\n",
    "        mean_threshold = mean_changes.quantile(0.8)\n",
    "        std_threshold = std_changes.quantile(0.8)\n",
    "        \n",
    "        breaks = []\n",
    "        for i in range(window, len(series)):\n",
    "            if (mean_changes.iloc[i] > mean_threshold or \n",
    "                std_changes.iloc[i] > std_threshold):\n",
    "                breaks.append(i)\n",
    "        \n",
    "        return breaks\n",
    "    \n",
    "    def segment_skus(self, method='combined'):\n",
    "        \"\"\"\n",
    "        Segment SKUs based on characteristics\n",
    "        Methods: 'statistical', 'business_logic', 'combined'\n",
    "        \"\"\"\n",
    "        if not self.sku_features:\n",
    "            self.calculate_sku_features()\n",
    "        \n",
    "        if method == 'statistical':\n",
    "            return self._statistical_segmentation()\n",
    "        elif method == 'business_logic':\n",
    "            return self._business_logic_segmentation()\n",
    "        else:\n",
    "            return self._combined_segmentation()\n",
    "    \n",
    "    def _statistical_segmentation(self):\n",
    "        \"\"\"Cluster SKUs based on statistical features\"\"\"\n",
    "        # Prepare feature matrix\n",
    "        feature_names = ['cv', 'trend_strength', 'seasonality_strength', \n",
    "                        'growth_rate', 'volatility', 'break_points']\n",
    "        \n",
    "        X = []\n",
    "        skus = []\n",
    "        \n",
    "        for sku, features in self.sku_features.items():\n",
    "            row = [features.get(f, 0) for f in feature_names]\n",
    "            \n",
    "            # Check for valid values (no inf or nan)\n",
    "            if all(np.isfinite(val) for val in row):\n",
    "                X.append(row)\n",
    "                skus.append(sku)\n",
    "        \n",
    "        if len(X) < 2:\n",
    "            # Not enough valid data for clustering\n",
    "            segments = {}\n",
    "            for sku in self.sku_features.keys():\n",
    "                segments[sku] = 'single_cluster'\n",
    "            self.segments = segments\n",
    "            return segments\n",
    "        \n",
    "        X = np.array(X)\n",
    "        \n",
    "        # Standardize features\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X)\n",
    "        \n",
    "        # Find optimal number of clusters\n",
    "        best_k = self._find_optimal_clusters(X_scaled)\n",
    "        \n",
    "        # Perform clustering\n",
    "        kmeans = KMeans(n_clusters=best_k, random_state=42)\n",
    "        clusters = kmeans.fit_predict(X_scaled)\n",
    "        \n",
    "        # Assign segments\n",
    "        segments = {}\n",
    "        for i, sku in enumerate(skus):\n",
    "            segments[sku] = f'statistical_cluster_{clusters[i]}'\n",
    "        \n",
    "        # Handle SKUs that were excluded from clustering\n",
    "        for sku in self.sku_features.keys():\n",
    "            if sku not in segments:\n",
    "                segments[sku] = 'outlier_cluster'\n",
    "        \n",
    "        self.segments = segments\n",
    "        return segments\n",
    "    \n",
    "    def _business_logic_segmentation(self):\n",
    "        \"\"\"Segment based on business logic\"\"\"\n",
    "        segments = {}\n",
    "        \n",
    "        for sku, features in self.sku_features.items():\n",
    "            # Get features with default values for missing/invalid data\n",
    "            volume_cat = features.get('volume_category', 'unknown')\n",
    "            lifecycle = features.get('lifecycle_stage', 'unknown')\n",
    "            seasonality = features.get('seasonality_strength', 0)\n",
    "            volatility = features.get('volatility', 0)\n",
    "            growth_rate = features.get('growth_rate', 0)\n",
    "            \n",
    "            # Handle infinite or NaN values\n",
    "            if not np.isfinite(seasonality):\n",
    "                seasonality = 0\n",
    "            if not np.isfinite(volatility):\n",
    "                volatility = 0\n",
    "            if not np.isfinite(growth_rate):\n",
    "                growth_rate = 0\n",
    "            \n",
    "            # Define segment based on characteristics\n",
    "            if seasonality > 0.3:\n",
    "                segment = f'seasonal_{volume_cat}'\n",
    "            elif growth_rate > 0.2:\n",
    "                segment = f'growth_{volume_cat}'\n",
    "            elif growth_rate < -0.2:\n",
    "                segment = f'decline_{volume_cat}'\n",
    "            elif volatility > 0.5:\n",
    "                segment = f'volatile_{volume_cat}'\n",
    "            else:\n",
    "                segment = f'stable_{volume_cat}'\n",
    "            \n",
    "            segments[sku] = segment\n",
    "        \n",
    "        self.segments = segments\n",
    "        return segments\n",
    "    \n",
    "    def _combined_segmentation(self):\n",
    "        \"\"\"Combine statistical and business logic approaches\"\"\"\n",
    "        # Start with business logic\n",
    "        business_segments = self._business_logic_segmentation()\n",
    "        \n",
    "        # Refine with statistical clustering within business segments\n",
    "        refined_segments = {}\n",
    "        \n",
    "        for segment_name in set(business_segments.values()):\n",
    "            skus_in_segment = [sku for sku, seg in business_segments.items() \n",
    "                             if seg == segment_name]\n",
    "            \n",
    "            if len(skus_in_segment) > 3:  # Only cluster if enough SKUs\n",
    "                # Extract features for this segment\n",
    "                X = []\n",
    "                valid_skus = []\n",
    "                \n",
    "                for sku in skus_in_segment:\n",
    "                    if sku in self.sku_features:\n",
    "                        features = self.sku_features[sku]\n",
    "                        row = [features['cv'], features['trend_strength'], \n",
    "                              features['volatility']]\n",
    "                        \n",
    "                        # Check for valid values (no inf or nan)\n",
    "                        if all(np.isfinite(val) for val in row):\n",
    "                            X.append(row)\n",
    "                            valid_skus.append(sku)\n",
    "                \n",
    "                if len(valid_skus) > 1:\n",
    "                    X = np.array(X)\n",
    "                    \n",
    "                    # Perform sub-clustering\n",
    "                    scaler = StandardScaler()\n",
    "                    X_scaled = scaler.fit_transform(X)\n",
    "                    \n",
    "                    n_clusters = min(3, len(valid_skus))\n",
    "                    if n_clusters > 1:\n",
    "                        kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "                        sub_clusters = kmeans.fit_predict(X_scaled)\n",
    "                        \n",
    "                        # Assign refined segments to valid SKUs\n",
    "                        for i, sku in enumerate(valid_skus):\n",
    "                            refined_segments[sku] = f'{segment_name}_sub{sub_clusters[i]}'\n",
    "                    else:\n",
    "                        # If only one cluster, keep original segment name\n",
    "                        for sku in valid_skus:\n",
    "                            refined_segments[sku] = segment_name\n",
    "                    \n",
    "                    # Assign original segment name to invalid SKUs\n",
    "                    for sku in skus_in_segment:\n",
    "                        if sku not in valid_skus:\n",
    "                            refined_segments[sku] = segment_name\n",
    "                else:\n",
    "                    # Not enough valid SKUs for clustering\n",
    "                    for sku in skus_in_segment:\n",
    "                        refined_segments[sku] = segment_name\n",
    "            else:\n",
    "                # Not enough SKUs in segment for sub-clustering\n",
    "                for sku in skus_in_segment:\n",
    "                    refined_segments[sku] = segment_name\n",
    "        \n",
    "        self.segments = refined_segments\n",
    "        return refined_segments\n",
    "    \n",
    "    def _find_optimal_clusters(self, X):\n",
    "        \"\"\"Find optimal number of clusters using silhouette score\"\"\"\n",
    "        silhouette_scores = []\n",
    "        K_range = range(2, min(10, len(X)))\n",
    "        \n",
    "        for k in K_range:\n",
    "            kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "            labels = kmeans.fit_predict(X)\n",
    "            score = silhouette_score(X, labels)\n",
    "            silhouette_scores.append(score)\n",
    "        \n",
    "        return K_range[np.argmax(silhouette_scores)]\n",
    "    \n",
    "    def analyze_segments(self):\n",
    "        \"\"\"Analyze characteristics of each segment\"\"\"\n",
    "        if not self.segments:\n",
    "            print(\"No segments found. Run segment_skus() first.\")\n",
    "            return\n",
    "        \n",
    "        segment_analysis = {}\n",
    "        \n",
    "        for segment_name in set(self.segments.values()):\n",
    "            skus_in_segment = [sku for sku, seg in self.segments.items() \n",
    "                             if seg == segment_name]\n",
    "            \n",
    "            # Calculate segment characteristics\n",
    "            characteristics = {\n",
    "                'sku_count': len(skus_in_segment),\n",
    "                'avg_volume': np.mean([self.sku_features[sku]['mean_volume'] \n",
    "                                     for sku in skus_in_segment]),\n",
    "                'avg_cv': np.mean([self.sku_features[sku]['cv'] \n",
    "                                 for sku in skus_in_segment]),\n",
    "                'avg_seasonality': np.mean([self.sku_features[sku]['seasonality_strength'] \n",
    "                                          for sku in skus_in_segment]),\n",
    "                'avg_growth_rate': np.mean([self.sku_features[sku]['growth_rate'] \n",
    "                                          for sku in skus_in_segment]),\n",
    "                'skus': skus_in_segment\n",
    "            }\n",
    "            \n",
    "            segment_analysis[segment_name] = characteristics\n",
    "        \n",
    "        self.segment_characteristics = segment_analysis\n",
    "        return segment_analysis\n",
    "    \n",
    "    def recommend_forecasting_models(self):\n",
    "        \"\"\"Recommend appropriate forecasting models for each segment\"\"\"\n",
    "        if not self.segment_characteristics:\n",
    "            self.analyze_segments()\n",
    "        \n",
    "        recommendations = {}\n",
    "        \n",
    "        for segment, chars in self.segment_characteristics.items():\n",
    "            models = []\n",
    "            \n",
    "            # Based on segment characteristics\n",
    "            if chars['avg_seasonality'] > 0.3:\n",
    "                models.extend(['Prophet', 'SARIMA', 'Holt-Winters'])\n",
    "            \n",
    "            if chars['avg_cv'] > 0.8:  # High variability\n",
    "                models.extend(['Prophet', 'LSTM', 'Random Forest'])\n",
    "            \n",
    "            if chars['avg_growth_rate'] > 0.2:  # Strong growth\n",
    "                models.extend(['Prophet', 'Linear Regression with Trend'])\n",
    "            \n",
    "            if chars['sku_count'] < 5:  # Small segment\n",
    "                models.extend(['Simple Exponential Smoothing', 'Linear Regression'])\n",
    "            \n",
    "            # Default models\n",
    "            if not models:\n",
    "                models = ['ARIMA', 'Simple Exponential Smoothing']\n",
    "            \n",
    "            recommendations[segment] = list(set(models))\n",
    "        \n",
    "        return recommendations\n",
    "    \n",
    "    def plot_segments(self, figsize=(15, 10)):\n",
    "        \"\"\"Visualize segments and their characteristics\"\"\"\n",
    "        if not self.segments:\n",
    "            print(\"No segments found. Run segment_skus() first.\")\n",
    "            return\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=figsize)\n",
    "        \n",
    "        # Prepare data for plotting\n",
    "        segment_data = []\n",
    "        for sku, segment in self.segments.items():\n",
    "            features = self.sku_features[sku]\n",
    "            segment_data.append({\n",
    "                'sku': sku,\n",
    "                'segment': segment,\n",
    "                'volume': features['mean_volume'],\n",
    "                'cv': features['cv'],\n",
    "                'seasonality': features['seasonality_strength'],\n",
    "                'growth_rate': features['growth_rate']\n",
    "            })\n",
    "        \n",
    "        df = pd.DataFrame(segment_data)\n",
    "        \n",
    "        # Plot 1: Volume vs CV by segment\n",
    "        sns.scatterplot(data=df, x='volume', y='cv', hue='segment', ax=axes[0,0])\n",
    "        axes[0,0].set_title('Volume vs Coefficient of Variation')\n",
    "        axes[0,0].set_xscale('log')\n",
    "        \n",
    "        # Plot 2: Seasonality vs Growth Rate by segment\n",
    "        sns.scatterplot(data=df, x='seasonality', y='growth_rate', hue='segment', ax=axes[0,1])\n",
    "        axes[0,1].set_title('Seasonality vs Growth Rate')\n",
    "        \n",
    "        # Plot 3: Segment distribution\n",
    "        segment_counts = df['segment'].value_counts()\n",
    "        axes[1,0].bar(range(len(segment_counts)), segment_counts.values)\n",
    "        axes[1,0].set_xticks(range(len(segment_counts)))\n",
    "        axes[1,0].set_xticklabels(segment_counts.index, rotation=45)\n",
    "        axes[1,0].set_title('SKUs per Segment')\n",
    "        \n",
    "        # Plot 4: Average characteristics by segment\n",
    "        segment_summary = df.groupby('segment')[['volume', 'cv', 'seasonality', 'growth_rate']].mean()\n",
    "        segment_summary.plot(kind='bar', ax=axes[1,1])\n",
    "        axes[1,1].set_title('Average Characteristics by Segment')\n",
    "        axes[1,1].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Example usage\n",
    "'''\n",
    "def create_sample_data():\n",
    "    \"\"\"Create sample data in long format for demonstration\"\"\"\n",
    "    np.random.seed(42)\n",
    "    dates = pd.date_range('2022-01-01', periods=36, freq='MS')\n",
    "    \n",
    "    # Create different types of SKUs\n",
    "    skus_data = []\n",
    "    \n",
    "    # Seasonal SKU\n",
    "    seasonal_pattern = 50 + 20 * np.sin(2 * np.pi * np.arange(36) / 12)\n",
    "    seasonal_orders = seasonal_pattern + np.random.normal(0, 5, 36)\n",
    "    \n",
    "    # Growth SKU\n",
    "    growth_trend = 100 + 5 * np.arange(36)\n",
    "    growth_orders = growth_trend + np.random.normal(0, 10, 36)\n",
    "    \n",
    "    # Volatile SKU\n",
    "    volatile_base = 200 + np.random.normal(0, 50, 36)\n",
    "    volatile_orders = np.abs(volatile_base)\n",
    "    \n",
    "    # Stable SKU\n",
    "    stable_orders = 150 + np.random.normal(0, 8, 36)\n",
    "    \n",
    "    # Decline SKU\n",
    "    decline_trend = 300 - 5 * np.arange(36)\n",
    "    decline_orders = decline_trend + np.random.normal(0, 15, 36)\n",
    "    \n",
    "    # Create long format data\n",
    "    all_data = []\n",
    "    \n",
    "    sku_patterns = {\n",
    "        'SKU001': seasonal_orders,\n",
    "        'SKU002': growth_orders,\n",
    "        'SKU003': volatile_orders,\n",
    "        'SKU004': stable_orders,\n",
    "        'SKU005': decline_orders\n",
    "    }\n",
    "    \n",
    "    for sku, orders in sku_patterns.items():\n",
    "        for i, date in enumerate(dates):\n",
    "            all_data.append({\n",
    "                'SKU': sku,\n",
    "                'date': date.strftime('%Y-%m-%d'),\n",
    "                'actual_orders': max(0, orders[i])  # Ensure non-negative\n",
    "            })\n",
    "    \n",
    "    return pl.DataFrame(all_data)\n",
    "\n",
    "# Demo\n",
    "if __name__ == \"__main__\":\n",
    "    # Create sample data in long format\n",
    "    sample_data = create_sample_data()\n",
    "    print(\"Sample data structure:\")\n",
    "    print(sample_data.head())\n",
    "    \n",
    "    # Initialize segmentation\n",
    "    segmenter = TimeSeriesSegmentation(sample_data)\n",
    "    \n",
    "    # Calculate features\n",
    "    features = segmenter.calculate_sku_features()\n",
    "    print(\"\\nSKU Features:\")\n",
    "    for sku, feat in features.items():\n",
    "        print(f\"{sku}: CV={feat['cv']:.2f}, Seasonality={feat['seasonality_strength']:.2f}, Growth={feat['growth_rate']:.2f}\")\n",
    "    \n",
    "    # Segment SKUs\n",
    "    segments = segmenter.segment_skus(method='combined')\n",
    "    print(\"\\nSegments:\")\n",
    "    for sku, segment in segments.items():\n",
    "        print(f\"{sku}: {segment}\")\n",
    "    \n",
    "    # Analyze segments\n",
    "    analysis = segmenter.analyze_segments()\n",
    "    print(\"\\nSegment Analysis:\")\n",
    "    for segment, chars in analysis.items():\n",
    "        print(f\"{segment}: {chars['sku_count']} SKUs, Avg Volume: {chars['avg_volume']:.0f}\")\n",
    "    \n",
    "    # Get model recommendations\n",
    "    recommendations = segmenter.recommend_forecasting_models()\n",
    "    print(\"\\nModel Recommendations:\")\n",
    "    for segment, models in recommendations.items():\n",
    "        print(f\"{segment}: {models}\")\n",
    "    \n",
    "    # Plot segments\n",
    "    segmenter.plot_segments()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19b5b4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "idf = pl.read_parquet(\"C:\\\\Users\\\\smishra14\\\\setup\\\\repos\\\\fcst\\\\data\\\\Trauma.parquet\")\n",
    "df= idf.filter(pl.col('SALES_DATE')<=datetime.today()-relativedelta(months=1))\n",
    "\n",
    "# Initialize segmentation (adjust column names as needed)\n",
    "segmenter = TimeSeriesSegmentation(\n",
    "    data=df.with_columns(pl.col('SALES_DATE').cast(pl.Utf8),pl.col('`Act Orders Rev').cast(pl.Float32)),\n",
    "    sku_col='CatalogNumber',     # your SKU column name\n",
    "    date_col='SALES_DATE',          # your date column name  \n",
    "    value_col='`Act Orders Rev' # your orders column name\n",
    ")\n",
    "\n",
    "# Rest of the workflow remains the same\n",
    "segments = segmenter.segment_skus(method='combined')\n",
    "analysis = segmenter.analyze_segments()\n",
    "recommendations = segmenter.recommend_forecasting_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cae18fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.from_dict(segments).transpose(include_header=True,header_name='CatalogNumber',column_names=[\"segment\"]).write_csv(\"segments.csv\")\n",
    "ff=pl.DataFrame()\n",
    "for i in analysis.keys():\n",
    "    dff=pl.from_dict(analysis[i])\n",
    "    dff=dff.with_columns(segment=pl.lit(i))\n",
    "    ff=pl.concat([ff,dff],how='diagonal_relaxed')\n",
    "ff.write_csv(\"analysis.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4da7192",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (9, 28)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>statistic</th><th>Selling Division</th><th>Area</th><th>Stryker Group Region</th><th>Region</th><th>Country</th><th>CatalogNumber</th><th>Business Sector</th><th>Business Unit</th><th>Franchise</th><th>Product Line</th><th>IBP Level 5</th><th>IBP Level 6</th><th>IBP Level 7</th><th>SALES_DATE</th><th>UOM</th><th>Pack Content</th><th>`L0 ASP Final Rev</th><th>`Act Orders Rev</th><th>Act Orders Rev Val</th><th>L2 DF Final Rev</th><th>L1 DF Final Rev</th><th>L0 DF Final Rev</th><th>L2 Stat Final Rev</th><th>`Fcst DF Final Rev</th><th>`Fcst Stat Final Rev</th><th>`Fcst Stat Prelim Rev</th><th>Fcst DF Final Rev Val</th></tr><tr><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>&quot;count&quot;</td><td>&quot;734160&quot;</td><td>&quot;734160&quot;</td><td>&quot;734160&quot;</td><td>&quot;734160&quot;</td><td>&quot;734160&quot;</td><td>&quot;734160&quot;</td><td>&quot;734160&quot;</td><td>&quot;734160&quot;</td><td>&quot;734160&quot;</td><td>&quot;734160&quot;</td><td>&quot;734160&quot;</td><td>&quot;734160&quot;</td><td>&quot;734160&quot;</td><td>&quot;734160&quot;</td><td>734160.0</td><td>734160.0</td><td>594965.0</td><td>205935.0</td><td>205137.0</td><td>467487.0</td><td>564335.0</td><td>565647.0</td><td>458528.0</td><td>305900.0</td><td>251045.0</td><td>242777.0</td><td>734160.0</td></tr><tr><td>&quot;null_count&quot;</td><td>&quot;0&quot;</td><td>&quot;0&quot;</td><td>&quot;0&quot;</td><td>&quot;0&quot;</td><td>&quot;0&quot;</td><td>&quot;0&quot;</td><td>&quot;0&quot;</td><td>&quot;0&quot;</td><td>&quot;0&quot;</td><td>&quot;0&quot;</td><td>&quot;0&quot;</td><td>&quot;0&quot;</td><td>&quot;0&quot;</td><td>&quot;0&quot;</td><td>0.0</td><td>0.0</td><td>139195.0</td><td>528225.0</td><td>529023.0</td><td>266673.0</td><td>169825.0</td><td>168513.0</td><td>275632.0</td><td>428260.0</td><td>483115.0</td><td>491383.0</td><td>0.0</td></tr><tr><td>&quot;mean&quot;</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>&quot;2025-02-14 12:47:59.999999&quot;</td><td>0.988121</td><td>1.077803</td><td>969.030499</td><td>36.945133</td><td>13009.980796</td><td>30.314416</td><td>25.154556</td><td>25.005851</td><td>29.740354</td><td>22.258892</td><td>27.22214</td><td>27.55798</td><td>3525.598852</td></tr><tr><td>&quot;std&quot;</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>0.097875</td><td>1.095921</td><td>1773.848025</td><td>191.638396</td><td>65359.091245</td><td>182.161462</td><td>166.960894</td><td>167.7697</td><td>177.159653</td><td>167.118273</td><td>181.95925</td><td>180.254423</td><td>40291.481297</td></tr><tr><td>&quot;min&quot;</td><td>&quot;CMF&quot;</td><td>&quot;United States&quot;</td><td>&quot;UNITED STATES&quot;</td><td>&quot;United States&quot;</td><td>&quot;UNITED STATES&quot;</td><td>&quot;0011201&quot;</td><td>&quot;Orthopaedics and Spine&quot;</td><td>&quot;Trauma&quot;</td><td>&quot;Trauma and Extremities&quot;</td><td>&quot;External Fixation&quot;</td><td>&quot;4FUSION&quot;</td><td>&quot;115 TR&quot;</td><td>&quot;0.3 SPLIT FREEZE-DRIED TRAD ST…</td><td>&quot;2022-09-01 00:00:00&quot;</td><td>0.01</td><td>1.0</td><td>-24656.16</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>-3206.3</td></tr><tr><td>&quot;25%&quot;</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>&quot;2023-12-01 00:00:00&quot;</td><td>1.0</td><td>1.0</td><td>146.53509</td><td>1.0</td><td>407.41</td><td>0.296415</td><td>0.0</td><td>0.0</td><td>0.312807</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td></tr><tr><td>&quot;50%&quot;</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>&quot;2025-03-01 00:00:00&quot;</td><td>1.0</td><td>1.0</td><td>401.0</td><td>4.0</td><td>1981.0</td><td>2.345941</td><td>1.219896</td><td>1.078099</td><td>2.379283</td><td>0.090928</td><td>1.0</td><td>1.0</td><td>0.0</td></tr><tr><td>&quot;75%&quot;</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>&quot;2026-05-01 00:00:00&quot;</td><td>1.0</td><td>1.0</td><td>1467.0</td><td>16.0</td><td>7695.48</td><td>10.358045</td><td>7.320674</td><td>7.0</td><td>10.276866</td><td>4.484278</td><td>7.105101</td><td>7.590663</td><td>0.0</td></tr><tr><td>&quot;max&quot;</td><td>&quot;SUSTAINABILITY&quot;</td><td>&quot;United States&quot;</td><td>&quot;UNITED STATES&quot;</td><td>&quot;United States&quot;</td><td>&quot;UNITED STATES&quot;</td><td>&quot;XSEXF010101&quot;</td><td>&quot;Orthopaedics and Spine&quot;</td><td>&quot;Trauma&quot;</td><td>&quot;Trauma and Extremities&quot;</td><td>&quot;Trauma Biologics&quot;</td><td>&quot;XPRESS DR AND UDR&quot;</td><td>&quot;XPRESS INSTRUMENTS- Obs&quot;</td><td>&quot;XPRESS DR AND UDR INSTRUMENT K…</td><td>&quot;2027-08-01 00:00:00&quot;</td><td>1.0</td><td>100.0</td><td>75855.6068</td><td>6731.0</td><td>4.1151e6</td><td>8538.24313</td><td>8538.24313</td><td>9135.920149</td><td>7222.901113</td><td>9135.920149</td><td>7389.012162</td><td>7389.012162</td><td>5.5486e6</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (9, 28)\n",
       "┌───────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬──────────┐\n",
       "│ statistic ┆ Selling   ┆ Area      ┆ Stryker   ┆ … ┆ `Fcst DF  ┆ `Fcst     ┆ `Fcst     ┆ Fcst DF  │\n",
       "│ ---       ┆ Division  ┆ ---       ┆ Group     ┆   ┆ Final Rev ┆ Stat      ┆ Stat      ┆ Final    │\n",
       "│ str       ┆ ---       ┆ str       ┆ Region    ┆   ┆ ---       ┆ Final Rev ┆ Prelim    ┆ Rev Val  │\n",
       "│           ┆ str       ┆           ┆ ---       ┆   ┆ f64       ┆ ---       ┆ Rev       ┆ ---      │\n",
       "│           ┆           ┆           ┆ str       ┆   ┆           ┆ f64       ┆ ---       ┆ f64      │\n",
       "│           ┆           ┆           ┆           ┆   ┆           ┆           ┆ f64       ┆          │\n",
       "╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪══════════╡\n",
       "│ count     ┆ 734160    ┆ 734160    ┆ 734160    ┆ … ┆ 305900.0  ┆ 251045.0  ┆ 242777.0  ┆ 734160.0 │\n",
       "│ null_coun ┆ 0         ┆ 0         ┆ 0         ┆ … ┆ 428260.0  ┆ 483115.0  ┆ 491383.0  ┆ 0.0      │\n",
       "│ t         ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ mean      ┆ null      ┆ null      ┆ null      ┆ … ┆ 22.258892 ┆ 27.22214  ┆ 27.55798  ┆ 3525.598 │\n",
       "│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ 852      │\n",
       "│ std       ┆ null      ┆ null      ┆ null      ┆ … ┆ 167.11827 ┆ 181.95925 ┆ 180.25442 ┆ 40291.48 │\n",
       "│           ┆           ┆           ┆           ┆   ┆ 3         ┆           ┆ 3         ┆ 1297     │\n",
       "│ min       ┆ CMF       ┆ United    ┆ UNITED    ┆ … ┆ 0.0       ┆ 0.0       ┆ 0.0       ┆ -3206.3  │\n",
       "│           ┆           ┆ States    ┆ STATES    ┆   ┆           ┆           ┆           ┆          │\n",
       "│ 25%       ┆ null      ┆ null      ┆ null      ┆ … ┆ 0.0       ┆ 0.0       ┆ 0.0       ┆ 0.0      │\n",
       "│ 50%       ┆ null      ┆ null      ┆ null      ┆ … ┆ 0.090928  ┆ 1.0       ┆ 1.0       ┆ 0.0      │\n",
       "│ 75%       ┆ null      ┆ null      ┆ null      ┆ … ┆ 4.484278  ┆ 7.105101  ┆ 7.590663  ┆ 0.0      │\n",
       "│ max       ┆ SUSTAINAB ┆ United    ┆ UNITED    ┆ … ┆ 9135.9201 ┆ 7389.0121 ┆ 7389.0121 ┆ 5.5486e6 │\n",
       "│           ┆ ILITY     ┆ States    ┆ STATES    ┆   ┆ 49        ┆ 62        ┆ 62        ┆          │\n",
       "└───────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴──────────┘"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import polars as pl\n",
    "df=pl.read_parquet(r\"C:\\Users\\smishra14\\setup\\repos\\fcst\\data\\Trauma.parquet\")\n",
    "df.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a5a072",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pl.read_parquet(r\"C:\\Users\\smishra14\\setup\\repos\\fcst\\data\\Sterishield.parquet\")\n",
    "df=df.with_columns(cluster=pl.col('cluster').cast(pl.Utf8))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e9bfbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.sort('SALES_DATE').with_columns(cluster=pl.col(\"cluster\").forward_fill().backward_fill().over(\"unique_id\")).describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "49384c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write_parquet(r\"C:\\Users\\smishra14\\setup\\repos\\fcst\\data\\Sterishield.parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py313",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
